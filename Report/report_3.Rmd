---
title: "Project in Data Analytics for Decision Making"
author: "Leonard Philippossian, David Pitteloud"
date: "11/10/2019"
output: 
 rmdformats::readthedown:
    css: custom.css
    self_contained: true
    toc_depth: 3
bibliography: bibliography.bibtex
---
```{r,include=FALSE, echo = FALSE, warning=FALSE}
#This part contains all the libraries required to run all the functions

library("Boruta")
library("caret")
library("caTools")
library("DataExplorer")
library("DMwR")
library("doParallel") 
library("ggcorrplot")
library("glmnet")
library("Hmisc")
library("kableExtra")
library("knitr")
library("lubridate")
library("Matrix")
library("mice")
library("mlbench")
library("MLeval")
library("naivebayes")
library("prediction")
library("readr")
library("Rmisc")
library("rpart")
library("tidyverse")
library("VIM")
library("xgboost")

```


# Introduction

Printing is a subject that most people do not know about and honestly do not really care about. Most of us think that it is only about having characters drawn automatically on a sheet of paper. Most of us are also often annoyed by printers as they, unfortunately, have technical issues that we do not always understand. These problems we face every day at a domestic level are a very brief insight into the physical complexity of printing at an industrial level.
Printing is not only interesting from a technical point of view. We all learned in primary school, from an occidental perspective that printing was invented around the year 1450 by Gutenberg. However, we talked more about more the great technical innovation and the end of the profession of copyist than of the real impact of printing on society. 
In fact, printing enabled to freeze holy texts, as copyist used to add personal thoughts in the retranscription. More importantly, printing is the major reason for the transmission of knowledge across Europe which will result in the Renaissance period and the scientific revolution, that still influences society nowadays.

As said before the importance of printing is underrated. This innovation probably changed the face of the world, therefore it is passionating to work on a dataset related to that topic.

For this project, we received a dataset about a special way of printing, probably rotogravure. Rotogravure is used for very large printing, that needs high-quality reproduction. In our dataset, the dependent variable is called band_type. 
Not being expert into rotogravure processes, we had to make a few research and we learned that a band was a series of grooves that appears in the cylinder during printing ruining the process and the finished product. Once it happens, the printer needs to be shut down and to be fixed. Apparently, it takes quite a long time to go through this repair process. It appears that this issue is relatively costly, as the final product is ruined and operators have to focus on repairing the printer. It is a problem because normally customers expect their product to be delivered on time, and printing jobs are usually very time-sensitive, for example, a newspaper cannot be delivered 3 days later than the expected time. The loss of time caused by banding is a major problem in the rotogravure. 

The dataset is here to help us identify the pattern that may cause this banding. The objective is to build models to predict if regarding many features, there will be a band or not during the printing process.

The dataset available contains quite a lot of features, which have to be observed one by one. As we do not have a strong knowledge of the context, we may not be able to apply any heuristic methods, based on intuition to select the best variables for our models. Instead, we will apply statistical procedures to select the most appropriate variables. In fact, machine learning is a very useful tool in an unknown field of expertise to identify decision-making rules from data.


# Data cleaning and exploration

Firstly, we will proceed to data cleaning. Even if our models may be powerful, they require that data are in a tidy form. Therefore some conversions will have to be performed [@Provost:2013:DSB:2564781, p . 30].

Moreover, to understand our data, it is fundamental to explore them and to visualize them. Data exploration will give us a better understanding of the problematic and also enable us to achieve data cleaning. Both of these phases are intimately related. 


```{r, include= FALSE, echo = FALSE, warning=FALSE}

#import the database
bands3 <- read_delim("C:/Users/Admin/Documents/Decision_making/Data/bands3.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)
#create data frame

df <- as.data.frame(bands3)
summary(bands3)
```

First, we should look at every variable of the raw table.

* 1. Date, a timestamp when the printing was performed.
* 2. Cylinder number, most of the cylinder used are different as there are more than 515 different types.
* 3. Customer, around 300 different customers are present.
* 4. Job number, there is also a wide range of different job numbers, about 500.
* 5. Grain screened, there are two class yes or no.
* 6. Ink color, it seems to have only one class which key but it is not recorded every time the same way, sometimes with capital letters, sometimes not.
* 7. Proof on ctd ink, mostly composed of "no", there are also a few "yes" and missing values.
* 8. Blade mfg, one class in predominant "benton", there is only one occurrence of "uddelholm", and around 60 missing values.
* 9. Cylinder division, same problem as ink color, it seems to have one class.
* 10. Paper type, 3 class but mainly composed of 2 coated and uncoated, the last one is called super.
* 11. Ink type, a bit structured as the paper type with two main classes coated and uncoated and the last one called cover.
* 12. Direct steam, mainly composed of "no" and a few "yes" and missing values.
* 13. Solvent type, mainly composed of the occurrence "line", a few "naphta", a few "xylol", and around 60 missing values.
* 14. Type on cylinder, composed of "yes" and "no" but mainly "no"
* 15. Press type, four different types of presses
* 16. Press, around 90 different classes, but some occurrences are appearing a lot. 
* 17. Unit number, 7 different classes, but almost half of the observation take a value of "2"
* 18. Cylinder size, there is also a problem with capital letters but otherwise 3 classes and only a few missing values.
* 19. Paper mill location, 5 main classes and a lot of missing values.
* 20. Plating tank, 2 classes and a few missing values.
* 21. Proof cut, there are around 50 NA and values are between 25 and 72.5.
* 22. Viscosity, values are between 35 and 72, with only a few NA.
* 23. Caliper, between 0.133 and 0.5330 with around 30 NA.
* 24. Ink temperature, between 11.20 and 24.50.
* 25. Humidity, values are between 57 and 105.
* 26. Roughness, the minimum value is 0.05625 and maximum is 1.25. There are 30 NA's.
* 27. Blade pressure, from 16 to 17, with around 60 missing values.
* 28. Varnis pct, values are between 0 and 35.8, with 56 missing values.
* 29. Press speed, the minimum is 0 and the maximum is 2600, but the mean is around 1800, values do not seem to be normally distributed.
* 30. Ink pct, around 56 missing values, and values are from 41 to 76.90
* 31. Solvent pct, between 22 and 53.4 and 56 missing values.
* 32. ESA voltage, 57 missing values, most of the values seem to be close to 0.
* 33. ESA amperage, 55 missing values and same as above, most values seem to be 0.
* 34. Wax, the minimum is 0 and a maximum of 3.1. 
* 35. Hardener, the minimum is 0 and the maximum is 3
* 36. Roller durometer, values are between 28 and 60 but again there are around 55 missing values.
* 37. Current density, the minimum is 30 and the maximum 45.
* 38. Anode space ratio, between 83.33 and 117.86
* 39. Chrome content, values are from 90 to 100, but distribution within the variable is not normal. Most of the values are close from 100.
* 40. Band_type, binary outcome, composed by 2 class, "band" or "no_band"


After a careful look at each variable, we may proceed to the first changes on the database to make it look tidier and that even before any data visualisation. Another aspect we noticed is that the number of missing values is very often similar. Missing values should be therefore further investigated.


The first change that seems obvious to improve the quality of the dataset is to put every letter in lowercase.

```{r, include= FALSE, echo = FALSE, warning=FALSE}

#remove capital letter
df <-data.frame(lapply(df, function(v) {
  if (is.character(v)) return(tolower(v))
  else return(v)
}))

```

Secondly, we will modify the first variable, regarding the date, to make three different variables, the day, the month and the year. The variable day will also be transformed into the day week, as it would be more relevant. After this first transformation, we will have two more variables than at the beginning, as we create three new variables and delete the date variable.  
We clean costumers names manually due to the numerous typos. And we also simplify the cylinder_no by keeping only letters assuming the letter means a category. We took those decisions to reduce the numbers of factors and to facilitate calculations. 

```{r, include= FALSE, echo = FALSE, warning=FALSE}

#separate date and transform it to day to day

df$date <- ymd(df$date)	

#add weekday variable 	
df$day <- weekdays(as.Date(df$date))
df$month <-month(as.Date(df$date))	
df$year <- year(as.Date(df$date))

#reorder variables, et j'ai juste enlever la date parce que ça me fait tout buguer plus bas
df <- df[c(41,42,43,2:40)]

#Clean customers names

df[,5][df[,5]== "ntlwildlife"] <- "natlwildlife"
df[,5][df[,5]== "jcpenny"] <- "jcp"
df[,5][df[,5]== "colortil"] <- "colortile"
df[,5][df[,5]== "homeshop"] <- "homeshopping"
df[,5][df[,5]== "eckerd"] <- "eckerds"
df[,5][df[,5]== "hanovrhous"|df[,5]=="hanoverhse"|df[,5]=="hanovrhouse"|df[,5]=="hanhouse"] <- "hanoverhouse"
df[,5][df[,5]== "woolwrth"] <- "woolworth"
df[,5][df[,5]== "best"] <- "bestprod"
df[,5][df[,5]== "belk"] <- "belks"
df[,5][df[,5]== "abbypress"| df[,5]== "abbey"] <- "abbeypress"
df[,5][df[,5]== "globalequp"] <- "global"
df[,5][df[,5]== "casliving"] <- "casualliving"


#Simplify cylinder_no

df[,4] <- as.factor(gsub("\\d", "", df[,4], perl=FALSE))


```


By observing the dataset, we may see that there is a lot of question mark, which are missing values. The first step before choosing a method to replace them is to rename all of these questions marks as NA, which means non-available value.

Here is the head of the dataset after this first cleaning. 


```{r, echo = FALSE, warning=FALSE}
#replace ? by NA
df[df == "?" | df == "" ] <- NA

kable(head(df)) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


Before doing the data exploration, our last step will be to define continuous variables as quantitative in the dataset and the same for categorical variables as factors. This manipulation will enable us to perform a deeper visualisation of the data set. 

```{r, include= FALSE, echo = FALSE, warning=FALSE}

#define which variable are the factors and which are continuous

df[,c(1:22,42)] <- lapply(df[,c(1:22,42)], factor)
df[,c(23:41)] <- lapply(df[,c(23:41)], as.numeric)

#create subset to facilitate visualisation

factor_df <- df[,c(1:22,42)]
quant_df <- df[,c(23:42)]


```



## Continuous variables exploration 

Firstly we will explore and plot the quantitative data and look at what information could be graphically discovered.

These first plots show us the distribution of our variables and eventually outliers. After a first look, we may already notice that **ESA_amperage** and **ESA_voltage** will not be very useful for our models as they are almost only composed by a single value. 

```{r, echo= FALSE, warning = FALSE}

#histogram of all the quantitative variables
plot_histogram(quant_df, title= "Distribution of continuous variable", ggtheme = theme_classic(), theme_config =list( "aspect.ratio" = 1))

```

We can see that most variables aren't really evenly distributed. We cannot say with certainty more from this graph because we cannot see the link between each variable and the dependent variable. 



```{r, echo= FALSE, warning = FALSE}

plot_density_fun <- function(input.col, data = quant_df){ 
  p <- ggplot(data, aes(x = data[,input.col]))+
  geom_density(aes(color = band_type, fill = band_type), alpha = 0.4, show.legend = FALSE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(quant_df)[input.col]))+
  theme_classic()
return(p)
}

all.density.plot <- lapply(c(1:20), plot_density_fun)


multiplot(plotlist = all.density.plot, cols = 5)

```


Here we can see more interesting pieces of information. Its the density plot of all continuous variables and the different color represent wether it band or not. We can see that ESA_Amperage or ESA_voltage have a very uneven distribution. Those uneven distribution should not be much of a problem for prediction but we will scale and center all continuous variables just in case. 
We can also notice variables that could help to predict band_type. We can suppose that proof_cut, press_speed, viscosity, roughness and ink_temperature will be more important to predict band_type. 


The next step of our data exploration is to plot correlation to see if any of the independent variables are correlated. A high correlation between variable could lead to multicollinearity. Because a lot of data are not normally distributed we will use the Spearman correlation method to have a more accurate representation of correlations [@Mukaka, p . 69].


Multicollinearity is problematic as it may increase the variance of the coefficient of regression. This would lead to a difficult interpretation of the coefficient.

```{r, echo=FALSE, warning=FALSE}

corr_table <- cor(quant_df[,-20], use = "pairwise.complete.obs", method ="spearman")

ggcorrplot(corr_table, lab = TRUE, lab_size=2)

```

We can notice that there is no huge correlation between the variables and that we probably will not suffer from multicollinearity. The maximum correlation is between **ink_pct** and **varnish_pct** at the value of -0.6.


## Factorial variables exploration 

The same process is yet performed on factorial variables.
We firstly create histograms to see the distribution of each variable. 

```{r, echo= FALSE, warning= FALSE, error=FALSE}

#histogram of the factors
plot_bar_fun <- function(input.col, data = factor_df){ 
  p <- ggplot(data, aes(x = data[,input.col]))+
  geom_bar(aes(color = band_type, fill = band_type), alpha = 0.4, show.legend = FALSE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(data)[input.col]))+
  theme(legend.position = "none")+
  theme_classic()
return(p)
}

all.density.plot <- lapply(c(1:23), plot_bar_fun)

multiplot(plotlist = all.density.plot, cols = 5)


```

This multiplot helps us to remove variables that have only one level and also variables that have too many levels and that would not be relevant. 
Variables with only one factor are  **ink_color** and **cylinder_division**. As they do not have any predicting ability, they will be deleted.

On the other hand, some variables have too many factors will also be deleted to avoid overcomplexity and long-running models. Variable concerned is **job_number**.


```{r,include=FALSE, echo=FALSE,warning=FALSE}
#We will now delete the following variables ink_color, job_number, costumer and  cylinder_division 


df <- df[,-c(6,8,11)]


```

After the data visualisation, only 39 variables are remaining. However, there may not be the only variable that will be deleted, the algorithmic variable selection process has not started yet.


# Variable and feature selection

The variable selection has four mains advantages:

* improve the prediction abilities of predictors
* reduce storage requirements 
* facilitate data visualization and understanding 
* reduce utilization time of the models

Variable and features selection can be performed in many ways. They can be ranked with their correlation coefficient, there are subset selection methods, which contain wrappers and embedded methods [@Guyon:2003:IVF:944919.944968, p . 1166].

In R, some packages propose a method to perform such variable and features selections. We will show below how to it with the package *caret* and *boruta*.

First, before the final selection of variables, we should finish the data preparation phase by doing the perilous task of missing values imputations


## Missing data 

Missing values is a tricky subject to address. The most appropriate way to proceed would be to first have a look at the theoretical aspect. 
Missing values may be categorised in three types.

* *Missing completely at Random (MCAR)* : This case is when data are really missing in a random way. Missingness, in this case, does not depend on any information present in the database.
* *Missing at random (MAR)* : This time probability of missing data is dependent on the observations. Correlation between missing data and other variables exists.
* *Missing not at Random (MNAR)* : Finally the last type is when missingness depends on unobserved data rather than observed data. It implies that data are missing for a reason. 

*MNAR* is the worst type of missing data. It may lead to false results, whereas the two other types may lead to a decrease of the statistical ability [@Jadhav, p . 915] 



To have a better idea of the actual situation with missing data, a solution is to plot them.

```{r, echo = FALSE, warning=FALSE}

plot_missing(df)
```
This plot enable us to see variables where there are the most of missing values.  From this graph we can already emit the hypothesis that many variables are MNAR (missing not at random) because variables from grain_screened to blade_pressure all have around 10% of missing datas which is strange. 


```{r, echo = FALSE, warning=FALSE}

aggr(df, prop = T, numbers = T)

```

As we thought before we can find here some very curious pattern. We can be sure that for some variables data are MNAR. So we have to explore this further. We want to know if those missing variables have some significance for our study. 


```{r, echo = FALSE, warning=FALSE}

df <- df %>% mutate(nbr_of_NA = apply(df,1, function(x) sum(length(which(is.na(x))))))


df %>% ggplot( aes(x = df[,40]))+
  geom_bar(aes(color = band_type, fill = band_type), alpha = 0.4, show.legend = TRUE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  theme(legend.position = "none")+
  xlab("Number of NAs")+
  ylab("Number of observation")+
  theme_bw()


```

Here we may assert that effectively if there is more than 9 NA per observation the result will always be band. We have to seek further in this direction because it seems that we may have here a sampling bias which can give a wrong representation of the reality to the models and also misguide the imputations. 


```{r, echo = FALSE, warning=FALSE}

ratio_table <- df %>% 
  group_by(nbr_of_NA) %>% 
  dplyr::summarise(number_of_obs = dplyr::n(), ratio_of_band = sum(band_type=="band")/(sum(band_type=="band")+sum(band_type=="noband")))


kable(ratio_table) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```
We can see here three group depending the ratio of banding: 

* *Number of NA between 0 and 2 per obeservation* : We can find in this group a ratio of `r sum(unlist(sapply(c(1,2,3), function(x) (ratio_table[x,2]*ratio_table[x,3]))))/sum(unlist(sapply(c(1,2,3), function(x) (ratio_table[x,2]))))`
* *Number of NA between 3 and 7 per obeservation* : We can find in this group a ratio of `r sum(unlist(sapply(c(4,5,6,7,8), function(x) (ratio_table[x,2]*ratio_table[x,3]))))/sum(unlist(sapply(c(4,5,6,7,8), function(x) (ratio_table[x,2]))))`. This higher number is maybe only due to randomness due to the low number of observations.  
* *Number of NA between more than 9 per obeservation* : We can find in this group a ratio of `r sum(unlist(sapply(9:15, function(x) (ratio_table[x,2]*ratio_table[x,3]))))/sum(unlist(sapply(9:15, function(x) (ratio_table[x,2]))))`. 


Because we have no way to know the real-life ratio of banding we will assume that the first group shows a more accurate representation of the reality because it's where we have most of the observations. Thus expected banding banding ratio is `r sum(unlist(sapply(c(1,2,3), function(x) (ratio_table[x,2]*ratio_table[x,3]))))/sum(unlist(sapply(c(1,2,3), function(x) (ratio_table[x,2]))))`. In the second group, we observe a higher ratio but we will assume that its because the number of observation is too low to see the convergence of the ratio. 


```{r, echo = FALSE, warning=FALSE}

df_few_NA <- df[df$nbr_of_NA<10,]

df_many_NA <- df[df$nbr_of_NA>=10,]

plot_missing(df_few_NA)

aggr(df_few_NA, prop = T, numbers = T)

```
From this output and especially patterns in the second graph, we can not be sure that those data are totally MCAR or MAR but to avoid overcomplexity we will assume so. 


For the third group, the ratio is equal to one which means all observations are band_type = band. This is a very suspicious behaviour in a dataset and may signify that those data are misleading. 


To explore data with a lot of NAs we want first want to know which variables have a lot of NAs. We take variables with more than 9% of NAs. 

```{r, echo = FALSE, warning=FALSE}


columns = tibble("Columns_with_alot_NAs" = colnames(df[,colSums(is.na(df))>=nrow(df)*0.09]))

kable(columns) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))%>%
  scroll_box(width = "100%", height = "200px")
```

All those variables are continuous which is also a strange coïcidence. 


Now we want to know if observations with a lot of NAs can still give us some information (can be used in our model).

```{r, echo = FALSE, warning=FALSE}


df1 <- df %>% mutate(more_than_9_NA = as.factor(ifelse(rowSums(is.na(.))>9, 1,0)))

plot_NA_fun <- function(input.col, data = df1){ 
  p <- ggplot(df1, aes(x = df1[,input.col]))+
  geom_bar(aes(color = more_than_9_NA, fill = more_than_9_NA), alpha = 0.4, show.legend = TRUE) +
  scale_color_manual(values = c("#F57F6A", "#81B6EA"))+
  scale_fill_manual(values = c("#F57F6A", "#81B6EA"))+
  xlab(paste(colnames(df1)[input.col]))+
  theme_classic()
return(p)
}

all.density.plot <- lapply(c(3,9), plot_NA_fun)

multiplot(plotlist = all.density.plot, cols = 1)

```

After further analysis, we found suspects behaviours like all those observations are only recorded 1992 and later (but very few others observations are recorded after 1991) or also paper_type = super is also only found in those observations. Those observations seem to come from a different group and are added into the dataset only because it banded. So for those data, there is an obvious sampling bias. The sample selection bias problem is a problem in machine learning. Theoretically,  to create the training set observation are randomly drawn from the same dataset, and the same happens for the test set. However, in the real world, it is hard to respect this assumption as we do not control everything in the data gathering process [@Zadrozny, p . 3]. But this sampling bias is too important to be ignored. Using those data could first mislead imputations of others observations (for example impute paper_Type=super) and also give us misleading predictions results. If we use those data, our intuition is that the model will perform much better on the dataset than it can in real conditions  

Thus we will ignore observations with more than 9 NAs to run our model. Accuracy will maybe be lower but we believe that in this manner models will describe a better representation of the reality.  

```{r, echo = FALSE, warning=FALSE, include=FALSE}

df <- df_few_NA

```


### Imputation


Theoretically, there are two ways to deal with missing values.

The first option is to ignore observations with missing values and the second strategy is to consider imputation of missing values. However, there is a serious problem with this first method. Ignoring missing values would reduce the dataset size which is already very small.
In our case, we will proceed to a missing data imputation, which could be defined as a procedure where missing values are replaced with some plausible values. There are several methods to do a data imputation but they may be split into two main categories.
The first one is the single imputation, where one plausible value is imputed to replace the missing value. The kNN imputation will be tested in this report. The principle of this technique is to impute a value to the missing value from a similar record in the dataset.
The second option consists of replacing missing values by multiples missing values [@Jadhav, p . 918]


```{r, include= FALSE, echo = FALSE, warning=FALSE}


#knn imputation with mod for factor and median for continuous 

df_knnimput <- knnImputation(df_few_NA, meth = "median")  # perform knn mputation. Think the rename de_knnimp




```

After this transformation, we do not have missing values anymore. This way to modify non-available data is not the only approach that could be taken. We will now perform a mice imputation which is based on a decision tree and then compares what was the best option to compute our models.

```{r,warning=FALSE, echo=FALSE, include= FALSE}



#multiple imputation
tempData <- mice(df_few_NA,m=5,maxit=11,meth='cart',seed=500, ntree =10 )
summary(tempData)
df <- mice::complete(tempData)


```

After some explorations, we decided that the mice imputation perform slightly better so we will use this technic to impute our data. 


## Variables and feature selection

It may be thought that wider data sets are providing better prediction, but this is not right. In fact, many machine learning algorithms decrease in accuracy if the number of variables is significantly higher than the optimal number of features [@Kohavi:1997:WFS:270613.270627, p . 275]. We will try to reduce a bit the number of variables using two algorithms, Learning Vector Quantization and Boruta.

To start this data selection process we have previously done a very trivial analysis, based on ranking with correlation coefficients. But as seen above, there are no really large correlation coefficients, therefore no variable may be deleted so easily.



### Learning Vector Quantization 

The first algorithm is the Learning Vector Quantization algorithm which is an artificial neural network algorithm.

```{r, warning= FALSE, echo=FALSE}



#Learning Vector Quantization

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=7)
# train the model
model <- train(band_type~., data=df, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# plot importance
plot(importance)


```
From the Learning Vector Quantization variable selection, we could delete variables up to **nbr_of_NA** and maybe higher. To confirm those result we will use a second method: Boruta. 

### Boruta 

Boruta is an algorithm, with an iterative construction, based on random forest, which helps us to achieve a data selection. The main advantage of Boruta is that it is providing an unbiased and stable selection of important and non-important attributes [@RePEc:jss:jstsof:v:036:i11, p . 3]. 

```{r, warning=FALSE, echo=FALSE, message=FALSE}

# to ensure reproducibility
set.seed(10)

boruta_df <- Boruta(band_type~.,data = df, doTrace = 2, ntree =500, maxRuns=500)
 
boruta_plot <- plot(boruta_df, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta_df$ImpHistory),function(i)
boruta_df$ImpHistory[is.finite(boruta_df$ImpHistory[,i]),i])
names(lz) <- colnames(boruta_df$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta_df$ImpHistory), cex.axis = 0.7)



df_final <- df %>% select(-c( "solvent_type", "blade_mfg", "direct_steam","ESA_amperage","plating_tank","day","nbr_of_NA","chrome_content", "customer"))
```

```{r, warning= FALSE, echo= FALSE}

boruta_plot

```

From this second method, we find similar results but not exactly so we decided to only delete variables that were ranked low in both selection and customer variable that was low only in the first method. We decided to delete it nevertheless because after some exploration we found out that deleting it has almost no impact on the prediction.  

Thus we decided to delete **ESA_amperage**, **blade_mfg**,**direct_steam**,**solvent_type**,**plating_tank**,**nbr_of_NA**,**chrome_content** and **customer**.

                                              


# Generals settings for all models  



k-Fold Cross-Validation

There are different ways to perform cross-validations. Usually, the database is split into a training and testing set. However, in our case, we decided to us the k-Fold Cross-Validation method. This approach is a bit different as in the cross-validation. Here the database is split into k groups. The first group is considered to be the validation set and the rest is used as training. The process is then repeated k times and every time the validation set changes. Statistically speaking the k-fold is calculated as follow :$$CV_{k}=\frac{1}{k}\sum_{i = 1}^k MSE_{i}$$ [@James:2014:ISL:2517747, p . 181]


Formally, the choice of k is usually 5 or 10, but it is not an established rule. Having a larger k enable to reduce the difference in size between validation and training set. As both sets are getting similar, the bias of the technique is decreasing [@Kuhn, p . 70].


We selected this approach because of the size of the dataset and the problem of variance resulting from a classical validation set method. The problem of variance may be illustrated as follow: if we split a dataset at random into two parts, results may be very different from a set to another and more especially with a dataset with only a few observations. Another aspect of the k-fold is that it is conservative. Therefore we do not overestimate our accuracy.



# ROC

It is always interesting to draw profit curves. However, to draw them, two main conditions should be fulfilled :
* The proportion of positive and negative instances should be known. In our case, we know that there is n% of positive and 1-n% of negative.
* The cost and benefits are also important features. Even if we do not have precise information about the cost-benefit matrix, we are aware that minimizing banding so positive is crucial.

A common approach to deal with uncertain conditions is the Receiver Operating Characteristics, (ROC) graph.
It is usually a two-dimension plot where the x-axis stands for false positive and true positive on the y-axis. It is a good way to compare different models, especially in a business perspective [@Provost:2013:DSB:2564781, p . 215]


# Accuracy

The accuracy is a simple measure to calculate the model performance. The calculation is as followed: accuracy = Number of correct decisions made / Total number of decisions made.
The error rate is the opposite of accuracy. To obtain we just have to subtract the accuracy in percent to 1. 

However the accuracy is not always the best way to measure model efficience, it may be a bit more complex. To overcome this problem, usually, a confusion matrix should be built. The role of such a matrix is to differentiate decisions performed by the classifier. In a problem with a two-class outcome like ours, there are two classes positive and negative. The matrix contains four categories:

* True positive, correctly predicted positive values.
* False positive, wrong positive prediction.
* True negative, correctly predicted negative values.
* False negative, wrong negative prediction.

Errors of the classifier are coming from false positive and false negative [@Provost:2013:DSB:2564781, p . 189]. From these observations, two measures of precision may be derivated :
* Sensitivity is a measure of the proportion of positives correctly predicted, it is simply calculated as the ratio of true positive and actual positive output.
* Specificity is a measure of the proportion of negatives correctly predicted, the calculation is the same as above, true negative divided by the total amount of negative values.


```{r,warning=FALSE, echo=FALSE}

#choose wich df to compute 

df <- df
#to make computation faster 

cl<-makeCluster(detectCores()) # detect and create a cluster
registerDoParallel(cl) # register it

#Pre-Compute CV folds so we can use the same ones for all models 
 
 set.seed(123)
 
 seeds <- vector(mode = "list", length = 51)
 for(i in 1:50) seeds[[i]] <- sample.int(1000, 22)
 
 ## For the last model:
 seeds[[51]] <- sample.int(1000, 1)
 
 
 
 fitControl <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats = 10,
                            savePredictions = TRUE,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary,
                            seeds= seeds
 )
 
 
 #preprocess 
 
 prePro <- c("center", "scale","YeoJohnson")
 
```
 

 
# Models 


```{r,warning=FALSE,echo=FALSE,include=FALSE}

load(file =  "C:/Users/Admin/Documents/Decision_making/Models/knn_fit.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/lasso_logreg_fit.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/Fit_naive_bayes.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/fit_cart.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/fit_rf.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/boosting_model.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/Fit_nnet.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/L_model.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/P_model.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/R_model.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/fit_bag.Rda")
load(file =  "C:/Users/Admin/Documents/Decision_making/Models/df_Final.Rda")

```


## KNN

K-nearest neighbours are one of the simplest classification technique in machine learning. It implies assigning to each new vector the same label as the closest example from the training set. The similarity is often measured thanks to euclidean distance. This model is fast to train but slow at making prediction [@Bishop:2006:PRM:1162264, p . 291]

In other words, a kNN can be used when there is little or no prior knowledge about the distribution of the data. The k in kNN in the number of neighbours that the algorithm will use, it should be normally an odd number to avoid equality in the vote. It is measuring the distance of defined features between observations or neighbours and the shortest distance will define from what the class of the unknown observation. The kNN will then take into account the k observations around the unknown value.


```{r,warning=FALSE, echo=FALSE}


#tuneGrid_knn = expand.grid(k = c(1,3,5,7,15,20))
#
#
#knn_fit <- train(
#  band_type ~ .,
#  data = df,
#  method = "knn",
#  preProcess = prePro,
#  trControl = fitControl,
#  tuneGrid = tuneGrid_knn
#)


plot(knn_fit)

best_knn_model <-
  knn_fit$results %>% 
  filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


```
We can see here that the best number of neighbours is around k=5 where we reach a ROC-AUC of 0.76. 



```{r,warning=FALSE, echo=FALSE}

kable(best_knn_model, caption = "Best KNN models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```
The table shows a different picture. We could argue here that k=1 is also a pretty good candidate because the sensitivity is 30% higher with almost the same specificity. 



## Lasso Logistic Regression 

Logistic regression is a linear model that enables to solve a two-class classification problem. From a purely statistical perspective, logistic regression should be more considered as a classification model rather than a regression. Mathematically speaking, under general assumptions, the probability of one to the binary outcome could be explicited as a logistic sigmoid implemented on a classic linear function with the vector $$\phi:p(C_{1}|\phi)=y(\phi)=\sigma(w^T\phi)$$ [@Bishop:2006:PRM:1162264, p . 205].

In simple words, the logistic regression is a binary classifier that enables to classify observations, regarding their dependent variables, by drawing a linear function.

The Lasso Logistic Regression is an improved version of the logistic regression where coefficients of less contributive variables are forced to be 0. The model will retain only significant variables. The table shows a different picture. We could argue here that k=1 is also a pretty good candidate because the sensitivity is 30% higher with almost the same specificity. 


```{r,warning=FALSE, echo=FALSE}


tunegrid_lasso_logreg <-
  expand.grid(
    alpha = c(0.1, 0.55, 1,1.5,2),
    lambda = c(0.00023, 0.0023, 0.023)
  )
#
##choose wich df to compute 
#lasso_logreg_fit <-
#  train(
#    band_type ~ .,
#    data = df,
#    method = 'glmnet',
#    trControl = fitControl,
#    tuneGrid = tunegrid_lasso_logreg
#)


plot(lasso_logreg_fit)

best_lasso_logreg_model <-
  lasso_logreg_fit$results %>% 
  filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 



```

Here we can see that a lambda of 0.002 is the best which mean a small constraint should be added to find $$\beta_i$$ and that there are some autocorrelations amongst variables (but not too much). 


```{r,warning=FALSE, echo=FALSE}

kable(best_lasso_logreg_model, caption = "Logistic Regression model result")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))


```
For the lasso logistic regression, we find a ROC of 0.79 with a sensitivity of 0.6. 



## Bayesian Model


The objective of Naive Bayes is to estimate the likelihood of a certain outcome. To do so, we first need to calculate the probability of each of our outcome. The calculation of the probability of an event to occur is as follow: posterior probability which represents the degree to which we believe a g given model accurately describes the situation is equal to the likelihood which describes how well the model predicts the data multiplied by the prior probability which describes the degree to which we believe the model accurately describes reality based on all of our prior information. That multiplication is then divided by normalizing constant that makes the posterior density
integrate to one. The main advantages are that Naive Bayes performs better than other models that may assume independence.

```{r,warning=FALSE, echo=FALSE}


 set.seed(123)
 
 seeds <- vector(mode = "list", length = 51)
 for(i in 1:50) seeds[[i]] <- sample.int(1000, 28)
 
 ## For the last model:
 seeds[[51]] <- sample.int(1000, 1)
 
 
 
 fitControl_bay <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats = 10,
                            savePredictions = TRUE,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary,
                            seeds= seeds
 )
 




#naive bayes 

tunegrid_naive_bayes <-
  expand.grid(
    laplace = c(0, 0.1, 1),
    adjust = c(0, 0.1, 1, 2, 3),
    usekernel = c(1)
  )

#Fit_naive_bayes <- train(
#  band_type ~ .,
#  data = df,
#  method = 'naive_bayes',
#  trControl = fitControl_bay,
#  tuneGrid = tunegrid_naive_bayes,
#  seed = 10,
#  verbose = FALSE
#)



best_naive_bayes_model <-
  Fit_naive_bayes$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE))


kable(best_naive_bayes_model[1:2,], caption = "Best naive bayes models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```
The best Naive Bayes model has a ROC of `r max(best_naive_bayes_model$ROC)`. But unfortunately, the sensitivity is low so Naive Bayes is not a good candidate for our data. 


## Tree-based methods

These methods consist of splitting predictors into different regions. They are recognised for their ease to be interpreted. From a performance perspective, classical trees are struggling to compete with supervised learning methods. Classical trees models are also subject to instability because the tree shape depends on the starting point. So different starting point will lead to a different tree(find local maxima: greedy algorithm). 
However, some ameliorated tree-based methods, involving the generation of multiple trees are able to compete in terms of accuracy. 



### CART 

In this exercise, we are predicting a qualitative output. Therefore, a classification tree is preferred over the regression tree. 
In a classification tree, a prediction is made that each observation belongs to the most commonly occurring class of training observation, in the region, it belongs [@James:2014:ISL:2517747, p . 311].

More basically growing a tree is based on several condition. The first is to choose what features will be important and what are the conditions for splits. Then it also needs to know when to stop splitting and finally pruning. The strategy to decide where a tree will be splitting is to group data in regions based on their similar traits decision. Finally, the tree split the nodes on all available variables and then select the split which result in the most homogenous subnodes.
Classification trees have the advantage to be easily leasible and to provide a graphical interpretation. The complexity parameter is used to control the size of the decision tree and to select the optimal tree size. The most common algorithms to build decision trees are Giri Index, Reduction in Variance and Chi-square


```{r,warning=FALSE, echo=FALSE}



tuneGrid_cart = expand.grid(cp = c(0,0.05,0.1,0.2,0.3,0.5))


#fit_cart <- train(
#  band_type ~ .,
#  data = df,
#  method = "rpart",
#  trControl = fitControl,
#  tuneGrid=tuneGrid_cart,
#  tuneLength = 10
#)

plot(fit_cart)
best_cart_model <-
  fit_cart$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 



```

Here we see that the lowest the complexity parameter, the better. We can suppose it is because there is a lot of variables with complex interactions. 


```{r,warning=FALSE, echo=FALSE}

rpart.plot::rpart.plot(fit_cart$finalModel)

```

Here we can observe the decision tree. The most decisive variable seems to be the factor press_type=woodhoe70. Press_Speed and humidity seem also very important. 



```{r,warning=FALSE, echo=FALSE}


kable(best_cart_model, caption = "Best CART models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

The best classification tree model has an accuracy of `r best_cart_model$ROC` with a cost of `r best_cart_model$cp`. 

We have talked about limits in a performance perspective with classical tree-based methods. A solution to make them more efficient is to aggregate many decision trees together to provide a more efficient predictive method. Those aggregated trees may be categorized into three different categories, bagging, random forest and boosting. 


### Bagging 

Bagging is a method that combines **B**oostrap and **agg**regation
In classification problems, the class predicted by each of the aggregated trees that we may call **B** trees are then weighted and the one that is the most appearing in the predictions is chosen. 
Bagging is an effective procedure to make more robust unstable classifiers. Bagging has an effect of reducing the variance by averaging a set of observations. Theoretically, it is more useful when used in very large datasets [@Büchlmann, p . 927]


```{r,warning=FALSE, echo=FALSE}


#fit_bag <- train(band_type ~ ., data = df, 
#                 method = "AdaBag", 
#                 trControl=fitControl,
#                 tuneSearch=3,
#                 seed = 10,
#                 preProcess = c("YeoJohnson","center", "scale"),
#                 verbose = FALSE)



plot(fit_bag)




best_bag_model <-
  fit_bag$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 




```

We can see here that after 100 trees there is no increase in ROCAUC and that the best depth is 3. We could emit the hypothesis it is because there are some meaningful interactions between variables. 

```{r,warning=FALSE, echo=FALSE}

kable(best_bag_model, caption = "Best random forest models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

We can see that the best bagging model have a ROC of `r max(best_bag_model$ROC)`. Unfortunately, the sensitivity is low with a score of 0.3 which mean is not a suitable model for our problem. 



### Random Forest  

Random Forest may be considered as an improved version of bagged trees as it reduces the correlation between each of the aggregated trees. The principle is relatively similar to bagging. Several trees are built on a bootstrapped training sample. However, during the process of creating trees and split in each tree, the random forest will not be able
to consider available predictors and therefore that reduces correlation within trees 

For example, if there is a very strong predictor in the model, bagged trees will only consider this particular strong variable at the top split. It means that all the aggregated trees will have a similar look and will result in correlated aggregated trees. Random Forest is dealing with this issue by forcing each split to consider only a subset of predictor [@James:2014:ISL:2517747, p . 319].

Roughly to summarise this difficult concept, in random forest we grow multiple trees. Then when an observation has to be classified, each tree roughly speaking vote for a class in which the object should be put in. The forest then is choosing the class that collected the most vote. One of the advantages of random forest is that it is handling missing values and mantains accuracy. Random forest are better at classifying than for regression which is a good point in our case. However unfortunately we have only very little control on what is happening within the model.

```{r,warning=FALSE, echo=FALSE}


tunegrid_rf <- expand.grid(.mtry=c(1,2,3,4,5,8,9,10,11,15,20))
#
#fit_rf <- train(band_type ~ ., data = df, 
#                 method = "rf", 
#                 trControl=fitControl,
#                 tuneGrid=tunegrid_rf,
#                 seed = 10,
#                 preProcess = c("YeoJohnson","center", "scale"),
#                 verbose = FALSE)



plot(fit_rf)

best_rf_model <-
  fit_rf$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 



```

We can see that the best number of randomly selected predictors at each split is 8 and then it decrease slowly. 


```{r,warning=FALSE, echo=FALSE}

kable(best_rf_model, caption = "Best random forest models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

The best random forest model has an ROC of `r max(best_rf_model$ROC)` with a mtry of `r best_rf_model %>% filter(ROC ==max(ROC, na.rm = TRUE)) %>%select(mtry)` but its mainly boosted by high specificity and a sensitivity that stay around 0.6. 

 ### Boosting

Once again we go back to the roots and to trees aggregation which was explained in the bagging. However, the difference stands in the way trees are built. In the case of boosting each tree is grown sequentially. That means that every new tree is built in a way that takes into account information about previously built trees. 
The idea of boosting is to create small trees and to learn step by step. The smaller the trees are, the more precise the model will be. However, models will be learning very slowly if trees are too small [@James:2014:ISL:2517747, p . 322].

```{r,warning=FALSE, echo=FALSE}
       
set.seed(123)
 
 seeds <- vector(mode = "list", length = 51)
 for(i in 1:50) seeds[[i]] <- sample.int(1000, 36)
 
 ## For the last model:
 seeds[[51]] <- sample.int(1000, 1)
 
 
 
 fitControl_boos <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats = 10, 
                            savePredictions = TRUE,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary,
                            seeds= seeds
 )
 

#boosting_model <- train(
#  band_type ~ .,
#  data = df,
#  method = 'xgbTree',
#  trControl = fitControl_boos,
#  seed = 10,
#  tuneLength=3
#)



best_boosting_model <-
  boosting_model$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


plot(boosting_model)


```

We can clearly see that the best depth is equal to 3. This is maybe because of there many hidden interactions between variables. 

```{r,warning=FALSE, echo=FALSE}

kable(best_boosting_model, caption = "Best Boostig models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

The best boosting model has a ROC of `r max(best_boosting_model$ROC)`. 


## Neural network

Neural network algorithms are inspired by the human brain structure. Basically, data are entering a neural network, train themselves, and then predict the output for a new dataset with similar data. 
Neural networks are formed by several layers of neurons. First, there is the input layer that receives information, and at the end, there is the output layer that will determine the output. Between these two layers, there are hidden layers.
All the neurons are connected to neurons of the next layer by channel, which are assigned a numerical value known as weight. When they are entering the networks, inputs are multiplied to the corresponding weight. In the hidden layer, each of the neurons has a numerical value, which is a bias. This value is added to the inputted sum. Then this value is passed through what we call the activation function. The result of the calculation will then decide if the neuron will be activated or not. This method is called forward propagation. For the final output it is a calculation of probability, what pattern is the most likely to occur. 
If the output is wrong, the neural network will itself adjust its weight to be more efficient. Those iterations, backward and forward will reinforce the strength of the model, even if it may take some time. 

```{r,warning=FALSE, echo=FALSE}

set.seed(123)
 
seeds <- vector(mode = "list", length = 51)
for(i in 1:50) seeds[[i]] <- sample.int(1000, 64)
 
## For the last model:
seeds[[51]] <- sample.int(1000, 1)
 
 
 
 fitControl_nnet <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats = 10,  
                            savePredictions = TRUE,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary,
                            seeds= seeds
 )
 
 
tunegrid_nnet <- expand.grid(size = c(1,2,3,4,5,8,10,15), decay = c(0, 0.1,0.2,0.3,0.4,0.5,0.6))

#
#Fit_nnet <- train(band_type ~ ., data = df, 
#                 method = "nnet", 
#                 trControl = fitControl_nnet,
#                 tuneGrid=tunegrid_nnet, 
#                 seed= 10,
#                 verbose = FALSE)



best_nnet_model <-
  Fit_nnet$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 




plot(Fit_nnet)
```

Here we can see that best models are produced with few hidden units and a weight decay of 0.2. 

```{r,warning=FALSE, echo=FALSE}

kable(best_nnet_model, caption = "Best neural network models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

The best neural net model has a ROC of `r max(best_nnet_model$ROC)` with a size of `r best_nnet_model %>% filter(ROC ==max(ROC, na.rm = TRUE)) %>%select(size)` and a decay of `r best_nnet_model %>% filter(ROC ==max(ROC, na.rm = TRUE)) %>%select(decay)`. 




## SVM

The support vector machine or commonly called (SVM) is popular for solving classification problems as in our case. One of the key property of this model is that parameters of the model are relative to a convex optimization model and therefore any local solution may be a global optimum. Support vector machine is a tool that helps taking decisions but it is not giving posterior probabilities [@Bishop:2006:PRM:1162264, p . 326].

More commonly, SVM may be used to do binary classification. In such procedure, the SVM finds a hyper-plane (line in 2d, a plane in 3d, etc) that separates its training data in such a way that the distance between the hyper-plane and the closest points from each class is maximized. Once a hyper-plane is defined new data points are classified in each side of the hyper-plane. 

The main advantage of SVM is that they may be used even if there a very few data, a bit like in our case. Usually when applying models on small data set the risk of overfitting is very high. However, SVM only uses a small part of data points to create the hyper-plane and does not really take into account if the dataset is large. The smaller the dataset is, the more accuracy we will lose. As said previously, the support vector machine will tell us in which hyper-plane will an observation will end, but unfortunately not the probability of observations to fall in a certain hyper-plane.


 ### SVM linear 

Theoretically, SVM can only be used when data are linearly separable, and the calculation explained above is performed. 


```{r,warning=FALSE, echo=FALSE}

tunegrid_L_model <- expand.grid(.C=c(0.01,0.1,0.5,1,2,3))
#
#L_model <- train(band_type ~ ., 
#                 data = df ,
#                 method="svmLinear",
#                 tuneGrid = tunegrid_L_model,
#                 trControl=fitControl, 
#                 seed = 10)
#

best_L_model <-
  L_model$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 




plot(L_model)
```

We can see that the best linear SVM model is at a cost of 2. And it reaches a pretty high ROCAUC of more than 0.8.



```{r,warning=FALSE, echo=FALSE}

kable(best_L_model, caption = "Best Linear SVM models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```
The best SVM linear model has a ROC of `r best_L_model$ROC` with a cost of `r best_L_model$C`. 


 ### SVM Polynomial 

However, it is possible to make data linearly separable even if it was not the case at the beginning. The solution stands into the dimension of the hyper-plane. If we are able to higher the dimension of the plane, we will be able to make this linear separation. This method is however complex from a computational aspect. The mapping proceeds through the use of kernels. By a combination of these kernels and a modification of parameters, it is very likely to obtain a better result.


```{r,warning=FALSE, echo=FALSE}

#Fit a Poly SVM

set.seed(123)
 
 seeds <- vector(mode = "list", length = 51)
 for(i in 1:50) seeds[[i]] <- sample.int(1000, 72)
 
 ## For the last model:
 seeds[[51]] <- sample.int(1000, 1)
 
 
 
fitControl_p <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats = 10, #To avoid too long computational time and bug 
                            savePredictions = TRUE,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary,
                            seeds= seeds)

tunegrid_P_model <- expand.grid(C=c(0.1,0.5,1,2), degree = c(1,2,3), scale =c(0.001,0.01,0.1,1,2,3))


#P_model <- train(band_type ~ ., 
#                             data = df, 
#                             method="svmPoly",
#                             tuneGrid = tunegrid_P_model,
#                             trControl=fitControl_p,
#                             seed = 10)


plot(P_model)
best_p_model <-
  P_model$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 







```

Here we can clearly see that the best degree is degree 1 which mean that vectors are linear. Cost doesn't seem to matter much at scale 2-3 and degree 1 (where the best fit is). 


```{r,warning=FALSE, echo=FALSE}


kable(best_p_model, caption = "Best poly SVM models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

The best SVM polynomial model has an accuracy of `r max(best_p_model$ROC)`.

### Radial SVM

In the radial SVM, the radial basis function kernel, or RBF kernel is used to map the new hyper-plane.

```{r,warning=FALSE, echo=FALSE}

set.seed(123)

seeds <- vector(mode = "list", length = 51)
 for(i in 1:50) seeds[[i]] <- sample.int(1000, 45)
 
 ## For the last model:
 seeds[[51]] <- sample.int(1000, 1)
 
 
 
fitControl_R <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats = 10,  
                            savePredictions = TRUE,
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary,
                            seeds= seeds)
#Fit a Radial SVM

tunegrid_R_model <-
  expand.grid(C = c(0.1, 0.25, 0.5, 1, 2),
              sigma = c(0,0.001, 0.01, 0.05, 0.1, 0.5, 1, 2,3))
#
#R_model <- train(
#  band_type ~ .,
#  data = df,
#  tuneGrid = tunegrid_R_model,
#  method = "svmRadial",
#  trControl = fitControl_R,
#  seed = 10
#)

best_R_model <-
  R_model$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


plot(R_model)

```

We can see that lower is the value of sigma the better is the model. This means that the radial model can easily overfit with those data thus it scores better when vectors are linear or close to linear (which it is when sigma is low). A cost of 1-2 seems to be the best here. 

```{r,warning=FALSE, echo=FALSE}

kable(best_R_model, caption = "Best Radial SVM models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

The best SVM radial model has a ROC of `r max(best_R_model$ROC)`. 


To conclude the SVM chapter it seems that the radial model is doing a lot less good than the linear and polynomial model. From those results, we can suggest the hypothesis that SVM models fit better on those data with linear vectors. 

# Models comparison 


```{r, warning=FALSE, echo=FALSE, message=FALSE, include= FALSE}

resamps <- resamples(list(KNN = knn_fit,
                          LassoLogReg = lasso_logreg_fit,
                          Deccision_Tree = fit_cart,
                          Naive_bayes=Fit_naive_bayes,
                          Random_forest=fit_rf,
                          Neural_net=Fit_nnet,
                          SVM_linear=L_model,
                          SMV_poly= P_model,
                          SVM_radial=R_model,
                          Bagging=fit_bag,
                          Boosting=boosting_model
                          ))

```

```{r, warning=FALSE, echo=FALSE, message=FALSE}

#choose wich df to compute 

bwplot(resamps, layout = c(3, 1))

```
Here we can compare all models. Using the ROC metric, we can see that the best model is Random Forest. It is particularly good because even the variance of the ROC metric seems low which means results are more robust. Using the Sensitivity metric results are more interesting. Boosting and, surprisingly, Lasso logistic Regression seems to do better than Random Forest (but at a cost of lower specificity). We can think that Lasso Logistic Regression is doing as good for two reasons. One is able to ignore totally irrelevant information ($\beta_{i}=0$), the second is to sacrifice some specificity. For boosting, it seems that the increase of sensitivity mainly only come from lower specificity. So now we can wonder what would be the sensitivity of the Random Forest if we reduce the specificity? To answer this question we can plot the ROC curve.


```{r, warning=FALSE, echo=FALSE, message=FALSE, include= FALSE}

#choose wich df to compute 

ROC_plot <- evalm(list(knn_fit,lasso_logreg_fit,fit_cart,fit_rf,Fit_nnet,L_model,P_model,R_model,boosting_model, fit_bag),gnames=c("KNN","Lasso Logistic Regression",'CART','Random Forest','Neural network', 'Linear SVM', 'Poly SVM','Radial SVM','Boosting',"Bagging"),rlinethick=0.6,fsize=8,plots='r')


```

```{r, warning=FALSE, echo=FALSE, message=FALSE}

#choose wich df to compute 

ROC_plot$roc


```

From this graph when we compare all models two are clearly outperforming others: Random Forest and Boosting, the worst model is Radial SVM by far. The Random Forest has a slightly better AUC (Area Under Curve) than the Boosting model. If it's important to have a low false-positive rate (high specificity) Boosting is better. For our case, we are more interested in having a high sensitivity (detecting band_type=band accurately) thus we will prefer a random forest model for prediction. To asses which trade-off should be done (defining the cutting point) we would need more information about cost for the printer for each case (cost of false-positive versus the cost of a false negative). 


Now we would like to know which variables have a lot of importance to determine whether or not there is banding or not. We will use the random forest result to explore that further. 

```{r,warning=FALSE, echo=FALSE}

Imp <- varImp(fit_rf)

Imp <- Imp$importance %>% tibble::rownames_to_column("Variable") %>%  arrange(desc(Overall))


kable(Imp, caption = "Most important variables to predict band_type")%>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  scroll_box(width = "100%", height = "500px")

```

So from this table we can see that **press_speed**, **viscosity**, **ink_temperature**, **solvent_pct** and **humidity** are really important. For factors **press_type = woodhoe70**, **cylinder_n = og**, **press = 815**, **unit_number = 7** and **ink_type = uncoated** seem pretty important. So we will plot those variables to have a better understanding of the underline relation. 


```{r,warning=FALSE, echo=FALSE}

best_var_continuous <- df %>% select("press_speed", "viscosity","ink_temperature","solvent_pct","humidity","band_type")


plot_density_fun <- function(input.col, data = best_var_continuous){ 
  p <- ggplot(data, aes(x = best_var_continuous[,input.col]))+
  geom_density(aes(color = band_type, fill = band_type), alpha = 0.4, show.legend = TRUE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(best_var_continuous)[input.col]))+
  theme_classic()
return(p)
}

all.density.plot <- lapply(c(1:5), plot_density_fun)

Rmisc::multiplot(plotlist = all.density.plot, cols = 2)
```

Here we can see some interesting behaviours. First, it seems that faster is the press the less it is likely to band, thus the recommended speed would be higher than 60. For the solvent_pct, levels lower than 40 seem more prone to band, thus we would recommend avoiding such levels. For the viscosity, it is levels higher than 25 that are more prone to band. The humidity variable is harder to distinguish but surely very low levels are bad (maybe there are interacting with other variables that make this variable more significant). For the ink temperature, it seems that either too low or too high level are prone to band, thus we would recommend to keep it around 45. 


```{r, echo = FALSE, warning=FALSE}

best_Factor <- list()

best_Factor[[1]] <-  df %>% filter(press_type == "woodhoe70") %>% select(press_type_woodhoe70="press_type", "band_type") %>% group_by(band_type) %>% dplyr::summarize(n=dplyr::n(), press_type_woodhoe70="press_type")

best_Factor[[2]] <- df %>% filter(cylinder_no == "g") %>% select(cylinder_no_g="cylinder_no", "band_type")%>% group_by(band_type) %>% dplyr::summarize(n=dplyr::n(),cylinder_no_g="cylinder_no" )

best_Factor[[3]] <- df %>% filter(press == "815") %>% select(press_815= "press", "band_type")%>% group_by(band_type) %>% dplyr::summarize(n=dplyr::n(), press_815= "press" )

best_Factor[[4]] <- df %>% filter(unit_number == "7") %>% select(unit_number_7= "unit_number", "band_type")%>% group_by(band_type) %>% dplyr::summarize(n=dplyr::n(), unit_number_7= "unit_number")

best_Factor[[5]] <- df %>% filter(cylinder_type == "yes") %>% select(cylinder_type_yes= "cylinder_type", "band_type")%>% group_by(band_type) %>% dplyr::summarize(n=dplyr::n(), cylinder_type_yes= "cylinder_type")

best_Factor[[6]] <- df %>% filter(ink_type == "uncoated") %>% select(ink_type_uncoated= "ink_type", "band_type")%>% group_by(band_type) %>% dplyr::summarize(n=dplyr::n(), ink_type_uncoated= "ink_type")



plot_bar_fun <- function(data){ 
  p <- ggplot(data, aes(x = band_type, y= n, color=band_type, fill = band_type))+
  geom_bar( stat="identity" , alpha = 0.6, show.legend = FALSE) +
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(data)[3]))+
  theme(legend.position = "none")+
  theme_classic()
return(p)
}

all.density.plot <- lapply(best_Factor, plot_bar_fun)

Rmisc::multiplot(plotlist = all.density.plot, cols = 2)


```

For factors, we can also find some interesting pieces of information. **Press_type = woodhoe70** was ranked the factor that has to most impact on prediction which seems strange from those graphs because **cylinder_type = yes** seems to be a lot more important. Maybe that is due to multicollinearity and interaction between variables that we can't see here. In any case, we can only recommend avoiding to have a cylinder_type = yes. For us, as statisticians, It's difficult to interpret more the meaning of those data but we hope that with a better understanding of rotogravure those results make sense and could help to reduce the percentage of banding. 


# Conclusion 












# References
