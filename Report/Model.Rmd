---
title: "Report"
author: "Leonard Philippossian David Pitteloud"
date: "11/10/2019"
output: 
 rmdformats::readthedown:
    css: custom.css
    self_contained: true
    toc_depth: 3
bibliography: bibliography.bibtex
---

```{r, include=FALSE, warning=FALSE, echo=FALSE}

#library("bartMachine")
library("caret")
library("caTools")
library("doParallel") 
library("mlbench")
library("MLeval")
library("naivebayes")
#library("rJava")
library("prediction")
library("xgboost")

```



```{r,warning=FALSE,echo=FALSE,include=FALSE}
load(file =  "full_df_knnimput.Rda")
load(file =  "boruta_df_knnimput.Rda")
load(file =  "lvq_df_knnimput.Rda")

```



#Generals settings for all models  

(expliquer pk 10 fold cv) (Expliquer aussi les metrics Accuracy + ROC)
```{r}

#choose wich df to compute 

df <- df_knnimput[,-c(2,4,5)] 


#to make computation faster 

cl<-makeCluster(detectCores()) # detect and create a cluster
registerDoParallel(cl) # register it

#Pre-Compute CV folds so we can use the same ones for all models (A justifier)

fitControl <- trainControl(method = "repeatedcv",
                           number = 2,
                           repeats = 2,
                           savePredictions = TRUE,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary
)

#preprocess 

prePro <- c("center", "scale")

```


#Models 

##KNN
```{r}

tuneGrid_knn = expand.grid(k = c(1,2,3,4,5,7,20))



knn_fit <- train(
  band_type ~ .,
  data = df,
  method = "knn",
  preProcess = prePro,
  trControl = fitControl,
  tuneGrid = tuneGrid_knn
)



plot(knn_fit)

best_knn_model <-
  knn_fit$results %>% 
  filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


kable(best_knn_model, caption = "Best KNN models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```




##logistic regression 

```{r}

#choose wich df to compute 


mod_fit <-
  train(
    band_type ~ .,
    data = df,
    method = "glm",
    family = "binomial",
    trControl = fitControl)


kable(mod_fit$results, caption = "Logistic Regression model result")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```


##Bayesian Model

```{r}

#naive bayes 

tunegrid_naive_bayes <-
  expand.grid(
    laplace = c(0, 0.1, 0.2, 1),
    adjust = c(0, 0.1, 1, 2, 3, 4, 5),
    usekernel = c(1)
  )

Fit_naive_bayes <- train(
  band_type ~ .,
  data = df,
  method = 'naive_bayes',
  trControl = fitControl,
  tuneGrid = tunegrid_naive_bayes,
  seed = 10,
  metric = "Accuracy",
  verbose = FALSE
)



best_naive_bayes_model <-
  Fit_naive_bayes$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) %>% slice(1:2)


kable(best_naive_bayes_model, caption = "Best naive bayes models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```



##CART 

```{r}

tuneGrid_cart = expand.grid(cp = c(0,0.05,0.1,0.2,0.3,0.5))


fit_cart <- train(
  band_type ~ .,
  data = df,
  method = "rpart",
  trControl = fitControl,
  tuneGrid=tuneGrid_cart,
  tuneLength = 10
)


best_cart_model <-
  fit_cart$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


kable(best_cart_model, caption = "Best CART models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))



```





##Random Forest  

```{r}


tunegrid_rf <- expand.grid(.mtry=c(1,2,3,4,5,8,10,20,30,40))

fit_rf <- train(band_type ~ ., data = df, 
                 method = "rf", 
                 trControl=fitControl,
                 tuneGrid=tunegrid_rf,
                 seed = 10,
                 preProcess = c("YeoJohnson","center", "scale"),
                 verbose = FALSE)



best_rf_model <-
  fit_rf$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


kable(best_rf_model, caption = "Best random forest models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```




##Neural net

```{r}

tunegrid_nnet <- expand.grid(size = c(1,2,3,4,5,8,10,15), decay = c(0, 0.1,0.2,0.3,0.4,0.5,0.6,0.7))


Fit_nnet <- train(band_type ~ ., data = df, 
                 method = "nnet", 
                 trControl = fitControl,
                 tuneGrid=tunegrid_nnet, 
                 seed= 10,
                 verbose = FALSE)


best_nnet_model <-
  Fit_nnet$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


kable(best_nnet_model, caption = "Best neural network models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```





##SVM

###SVM linear 

```{r}


#Fit a Linear SVM

tunegrid_L_model <- expand.grid(.C=c(0.01,0.1,0.5,1,2,3,4))

L_model <- train(band_type ~ ., 
                 data = df ,
                 method="svmLinear",
                 tuneGrid = tunegrid_L_model,
                 trControl=fitControl, 
                 seed = 10)

best_L_model <-
  L_model$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


kable(best_L_model, caption = "Best Linear SVM models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```




###SVM Polynomial 

```{r}

#Fit a Poly SVM

tunegrid_P_model <- expand.grid(C=c(0.1,0.5,1,2), degree = c(1,2,3), scale =c(0.001,0.01,0.1,1,2,3))


P_model <- train(band_type ~ ., 
                             data = df, 
                             method="svmPoly",
                             tuneGrid = tunegrid_P_model,
                             trControl=fitControl,
                             seed = 10)

best_p_model <-
  P_model$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


kable(best_p_model, caption = "Best poly SVM models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```




### Radial SVM


```{r}
            
#Fit a Radial SVM

tunegrid_R_model <-
  expand.grid(C = c(0.1, 0.25, 0.5, 1, 2),
              sigma = c(0.001, 0.01, 0.05, 0.1, 0.5, 1, 2,3))

R_model <- train(
  band_type ~ .,
  data = df,
  tuneGrid = tunegrid_R_model,
  method = "svmRadial",
  trControl = fitControl,
  seed = 10
)


best_R_model <-
  R_model$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


kable(best_R_model, caption = "Best Radial SVM models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```



##Boosting 

```{r}


boosting_model <- train(
  band_type ~ .,
  data = df,
  method = 'xgbTree',
  trControl = fitControl,
  seed = 10,
  tuneLength=3
)


best_boosting_model <-
  boosting_model$results %>% filter(Sens == max(Sens, na.rm = TRUE)| ROC == max(ROC, na.rm = TRUE)) 


kable(best_boosting_model, caption = "Best Boostig models using ROC and Sensitivity metrics")%>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


#Compare all models 

```{r}


ROC_plot <- evalm(list(Fit_naive_bayes,fit_cart,fit_rf,Fit_nnet,L_model,P_model,R_model,boosting_model),gnames=c('Naive bayes','CART','Random Forest','Neural network', 'Linear SVM', 'Poly SVM','Radial SVM','Boosting'),rlinethick=0.8,fsize=8,plots='r')

```

#Conclusion 

















