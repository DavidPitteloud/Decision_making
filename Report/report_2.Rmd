---
title: "Report"
author: "Leonard Philippossian David Pitteloud"
date: "11/10/2019"
output: 
 rmdformats::readthedown:
    css: custom.css
    self_contained: true
    toc_depth: 3
bibliography: bibliography.bibtex
---
```{r,include=FALSE, echo = FALSE, warning=FALSE}
#This part contains all the libraries required to run all the functions

library("Boruta")
library("caret")
library("DataExplorer")
library("DMwR")
library("ggcorrplot")
library("kableExtra")
library("knitr")
library("Hmisc")
library("lubridate")
library("mice")
library("mlbench")
library("readr")
library("Rmisc")
library("tidyverse")
library("VIM")



```



# Introduction

Printing is a subject that most people do not know about and minimize its importance. We all learned in primary school, from an occidental perspective that printing was invented around 1450 by Gutenberg. However, the focal point was more on the idea and the end of the profession of copyist than on its real impact. 
In fact, printing enable to freeze holy texts, as copyist used to add personnal thougts in the retranscription. More importantly, printing is the major reason for the transmission of knowledge which will result in the Renaissance period and the scientific revolution.

As said before printing is a underrated subject, that probably changed the face of the world, therefore it is passionating to work on a dataset related to that topic.

For this project, We received a dataset about a special way of printing, probably rotogravure. Rotogravure is used for very large printing, that need high quality reproduction. In our dataset, the dependant variable is called band_type. 
Not being  expert into rotogravure processes, we had to make a few researches and we learned that a band was a serie of grooves that appears in the cylinder during printing ruining the process and the finish product. Once it happens, the printers needs to be shut down and to be fixed. Apparently it takes quite a well to go through this repair process. Band is a problem for printers, and should be minimized.



The dataset is here to help us identify the pattern that may cause this band. The objective is to build model to predict if regarding many features, there will be a band or not on the finish product. 

The dataset available contains quite a lot of features, which have to observed one by one. As we do not have a strong knowledge of the context, we may not be able to apply any heuristic methods, based on intuition to select the best variables for our models. Instead, we will apply statistical procedures to select the most appropriate variables.


# Data cleaning and exploration

The first part will  be dedicated to data cleaning. Our models will be powerful but they require that datas are in a tidy form. Therefore some conversions will have to be made [@Provost:2013:DSB:2564781].

Moreover, to understand our data, it is fundamental to explore them, and to visualize them. Data exploration will give us a better understanding of the problematic and also enable us to achieve the data cleaning. Both of these phases are intimately related. 




```{r, include= FALSE, echo = FALSE, warning=FALSE}
#import the database
bands3 <- read_delim("bands3.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)
#create data frame

df <- as.data.frame(bands3)

```

After a quick look at the database we may proceed to the first changes on the database to make it look more tidy and that even before any data visualisation.


The first change that seems obvious to improve the quality of the dataset is to put every letter in lowercase.

```{r, include= FALSE, echo = FALSE, warning=FALSE}

#remove capital letter
df <-data.frame(lapply(df, function(v) {
  if (is.character(v)) return(tolower(v))
  else return(v)
}))

```

Secondly, we will modify the first variable, regarding the date, to make three different variables, the day, the month and the year. The variable day will also be transformed into the day week, as it would be more relevant. After this first transformation we will have two more variables than at the beginning, as we create three new variables and delete the date variable.  
We  clean costumers name manually due to the numourus typos. And we also simplify the cylinder_no by keeping only letters. We took those decisions to reduce the numbers of factors and to facilitate calculations. 

```{r, include= FALSE, echo = FALSE, warning=FALSE}

#separate date and transform it to day to day

df$date <- ymd(df$date)	

#add weekday variable 	
df$day <- weekdays(as.Date(df$date))
df$month <-month(as.Date(df$date))	
df$year <- year(as.Date(df$date))

#reorder variables, et j'ai juste enlever la date parce que ça me fait tout buguer plus bas
df <- df[c(41,42,43,2:40)]

#Clean customers names

df[,5][df[,5]== "ntlwildlife"] <- "natlwildlife"
df[,5][df[,5]== "jcpenny"] <- "jcp"
df[,5][df[,5]== "colortil"] <- "colortile"
df[,5][df[,5]== "homeshop"] <- "homeshopping"
df[,5][df[,5]== "eckerd"] <- "eckerds"
df[,5][df[,5]== "hanovrhous"|df[,5]=="hanoverhse"|df[,5]=="hanovrhouse"|df[,5]=="hanhouse"] <- "hanoverhouse"
df[,5][df[,5]== "woolwrth"] <- "woolworth"
df[,5][df[,5]== "best"] <- "bestprod"
df[,5][df[,5]== "belk"] <- "belks"
df[,5][df[,5]== "abbypress"| df[,5]== "abbey"] <- "abbeypress"
df[,5][df[,5]== "globalequp"] <- "global"
df[,5][df[,5]== "casliving"] <- "casualliving"


#Simplify cylinder_no

df[,4] <- as.factor(gsub("\\d", "", df[,4], perl=FALSE))


```


By observing the dataset, we may see that there a lot of question mark, which are missing values. The first step before choosing a method to replace them is to rename all of these questions marks as NA values, which means non available.

Here is the head of the dataset after this first cleaning. 

```{r, include= FALSE, echo = FALSE, warning=FALSE}
#replace ? by NA
df[df == "?" | df == "" ] <- NA

kable(head(df)) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


Peut-être inutile de préciser 
(Before doing the data exploration, our last step will be to define set continuous variables as quantitative in the dataset and the same for factors. This manipulation will enable us to perform a deeper visualisation of the data set.) 

```{r, include= FALSE, echo = FALSE, warning=FALSE}

#define which variable are the factors and which are continuous

df[,c(1:22,42)] <- lapply(df[,c(1:22,42)], factor)
df[,c(23:41)] <- lapply(df[,c(23:41)], as.numeric)

#create subset to facilitate visualisation

factor_df <- df[,c(1:22,42)]
quant_df <- df[,c(23:42)]


```



## Continuous variables exploration 

Firstly we will explore and plot the quantitative data and look what insight could be graphically discovered.

These first plot show us the distribution of our variables and maybe outliers. After a first look, we may already notice that **ESA_amperage** and **ESA_voltage** will not be very useful for our models as they are almost composed by a single value. 


```{r, echo= FALSE, warning = FALSE}

#histogram of all the quantitative variables

plot_histogram(quant_df)


```
We can see that most varaibles aren't really evenly distributed. We can't really say more from this graph because we can't see the link between each variable and the dependant variable. 



```{r, echo= FALSE, warning = FALSE}

plot_density_fun <- function(input.col, data = quant_df){ 
  p <- ggplot(data, aes(x = data[,input.col]))+
  geom_density(aes(color = band_type, fill = band_type), alpha = 0.4, show.legend = FALSE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(quant_df)[input.col]))+
  theme_classic()
return(p)
}

all.density.plot <- lapply(c(1:20), plot_density_fun)


multiplot(plotlist = all.density.plot, cols = 5)

```
Here we can see more interesting informations. First we can already notice variables that could help to predict band_type. We can suppose that proof_cut, press_speed, viscosity, roughness and ink_temperature will be more important to predict band_type. 


The next step is to plot correlation to see if any of the independent variables are correlated. A high correlation between variable could lead to multicollinearity. Because a lot of datas are not normally distributed we will use the spearman correlation method to have a more accurate representation of correlations. (mettre une citation lien https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/)
Multicollinearity is problematic as it may increase variance of the coefficient of regression. This would lead to a difficult interpretation of the coefficient.
```{r, echo=FALSE, warning=FALSE}

M <- cor(quant_df[,-20], use = "pairwise.complete.obs", method ="spearman")

ggcorrplot(M, lab = TRUE, lab_size=2)

```

We can notice that there is no huge correlation between the variables and that we probably will not suffer from multicollinearity. The maximum correlation is between **ink_pct** and **varnish_pct** at the value of -0.6.




## Factorial variables exploration 

The same process is yet perfomed on factorial variables.
We firstly create histograms to see the distribution of each variable. 

```{r, echo= FALSE, warning= FALSE, error=FALSE}

#histogram of the factors
plot_bar_fun <- function(input.col, data = factor_df){ 
  p <- ggplot(data, aes(x = data[,input.col]))+
  geom_bar(aes(color = band_type, fill = band_type), alpha = 0.4, show.legend = FALSE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(data)[input.col]))+
  theme(legend.position = "none")+
  theme_classic()
return(p)
}

all.density.plot <- lapply(c(1:23), plot_bar_fun)

multiplot(plotlist = all.density.plot, cols = 5)


```
This multiplot helps us to remove variable that have only one level and also variables that have to many level and that would not be relevant. 
Variables with only one factor are  **ink_color** and **cylinder_division**. As they do not have any predicting ability, they will be deleted.

On the other hand some variables have too many factors will also be deleted to avoid overcomplexity and long running models. Variables concerned are **job_number** and **costumer**.
The analysis of the quantitative and factorial variables enabled us to remove (On pourrait grouper les costumers similaires et créer des facteurs)

```{r,include=FALSE, echo=FALSE,warning=FALSE}
#We will now delete the following variables ink_color, job_number, costumer and  cylinder_division 


df <- df[,-c(6,8,11)]


```

After the data visualisation it remains only 39 variables. However there may not be the only variable that will be deleted, the variable selection process has not started yet.

```{r,include=FALSE, echo=FALSE,warning=FALSE}


#DF few na imputuée knn
df_fewNA_knimp<- knnImputation(df, meth = "median")
df_fewNA_knimp <- df_fewNA_knimp[,-5]


#DF few na imputuée mice cart
tempData <- mice(df,m=8,maxit=10,meth='cart',seed=500, ntree =10 )
df_fewNA_cart <- mice::complete(tempData)
df_fewNA_cart <- df_fewNA_cart[,-5]



#DF few na imputuée mice pmm 
tempData <- mice(df,m=8,maxit=10,meth='pmm',seed=500, ntree =10 )
df_fewNA_pmm <- mice::complete(tempData)
df_fewNA_pmm <- df_fewNA_pmm[,-5]




```



# Variable and feature selection

Variable selection has three many advantages:

* improve prediction abilities of predictors
* reduce storage requirements 
* facilitate data visualization and understanding 
* reduce utilization time of the models

Variable and features selection can be performed in many ways. They can be ranked with their correlation coefficient, there are subset selection methods, which contain wrappers and embedded methods [@Guyon:2003:IVF:944919.944968].

In R, there are packages that propose method to perform such variable and features selections. We will show below how to it with the package *carret* and *boruta*.

Let's finish the data preparation phase by doing the missing values imputations


## Missing data 

Missing values is a tricky subject to adress. The most appropriate way to proceed would be to first have a look of the theorical aspect. 
Missing values may be categorised in three types.

* *Missing completely at Random (MCAR)* : This case is when data are really missing in a random way. Missingness in this case does not depend on any inforation present in the database.
* *Missing at random (MAR)* : This time probability of missing data is dependent on the observations. Correlation between missing data and other variables exists.
* *Missing not at Random (MNAR)* : Finally the last type is when missingness depends on unobserved data rather than observed data. It implies that data are missing for a reason. 

*MNAR* is the worst type of missing data. It may lead to false results, whereas the two other types may lead to a decrease of the statistical ability [@Jadhav]



To have a better idea of the actual situation with missing datas, a solution is to plot them

```{r, echo = FALSE, warning=FALSE}

plot_missing(df)
```
This plot enable us to see variables where there are the most of missing values.  From this graph we can already emit the hypothesis that many variables are MNAR (missing not at random) because variables from grain_screened to blade_pressure all have around 10% of missing datas which is strange. 


```{r, echo = FALSE, warning=FALSE}

aggr(df, prop = T, numbers = T)

```
As we thought before we can find here some very strange pattern. We can be sure that for some variables datas are MNAR. So we have to explore this further. We want to know if those missing variables have some signifiance for our study. 


```{r, echo = FALSE, warning=FALSE}

df <- df %>% mutate(nbr_of_NA = apply(df,1, function(x) sum(length(which(is.na(x))))))


df %>% ggplot( aes(x = df[,40]))+
  geom_bar(aes(color = band_type, fill = band_type), alpha = 0.4, show.legend = TRUE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  theme(legend.position = "none")+
  xlab("Number of NAs")+
  ylab("Number of observation")+
  theme_bw()


```

Here we confirme that effectively if there is more than 9 NA per observation it's only band. We have to study this further because it seems that we may have here a sampling biais which can give a wrong representation of the reality to the models. 


```{r, echo = FALSE, warning=FALSE}

ratio_table <- df %>% 
  group_by(nbr_of_NA) %>% 
  dplyr::summarise(number_of_obs = dplyr::n(), ratio_of_band = sum(band_type=="band")/(sum(band_type=="band")+sum(band_type=="noband")))


kable(ratio_table) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```
We can see here three group depending the ratio of banding: 

* *Number of NA between 0 and 2 per obeservation* : We can find in this group a ratio of `r sum(unlist(sapply(c(1,2,3), function(x) (ratio_table[x,2]*ratio_table[x,3]))))/sum(unlist(sapply(c(1,2,3), function(x) (ratio_table[x,2]))))`
* *Number of NA between 3 and 7 per obeservation* : We can find in this group a ratio of `r sum(unlist(sapply(c(4,5,6,7,8), function(x) (ratio_table[x,2]*ratio_table[x,3]))))/sum(unlist(sapply(c(4,5,6,7,8), function(x) (ratio_table[x,2]))))`. This higher number is maybe only due to randomness due to the low number of observations.  
* *Number of NA between more than 9 per obeservation* : We can find in this group a ratio of `r sum(unlist(sapply(9:15, function(x) (ratio_table[x,2]*ratio_table[x,3]))))/sum(unlist(sapply(9:15, function(x) (ratio_table[x,2]))))`. 


Because we have no way to know the real life ratio of banding we will assume that the first group shows a more accurate representation of the reality bacause it's where we have most of observations. Thus expected banding banding ratio is `r sum(unlist(sapply(c(1,2,3), function(x) (ratio_table[x,2]*ratio_table[x,3]))))/sum(unlist(sapply(c(1,2,3), function(x) (ratio_table[x,2]))))`. In the second group we observe an higher ratio but we will assume that its because the number of observation is too low to see the convergence of the ratio. 

```{r, echo = FALSE, warning=FALSE}

df_few_NA <- df[df$nbr_of_NA<10,]

df_many_NA <- df[df$nbr_of_NA>=10,]

plot_missing(df_few_NA)

aggr(df_few_NA, prop = T, numbers = T)

```
From this output and and especially paterns in the second graph, we can't be sure that those datas are totally MCAR or MAR but to avoid overcomplexity we will assume so. 


For the third group the ratio is equal to one which means all observations are band_type = band. This is a very suspicious behavior in a dataset and may signify that those datas are misleading. 


To explore data with a lot of NAs we want first want to know which variables has a lot of NAs. We take variables with more than 9% of NAs. 
```{r, echo = FALSE, warning=FALSE}


columns = tibble("Columns_with_alot_NAs" = colnames(df[,colSums(is.na(df))>=nrow(df)*0.09]))

kable(columns) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

Now we want to know if observations with a lot of NAs can still give us some informations (can be used in our model).
```{r, echo = FALSE, warning=FALSE}


df1 <- df %>% mutate(more_than_9_NA = as.factor(ifelse(rowSums(is.na(.))>9, 1,0)))

plot_NA_fun <- function(input.col, data = df1){ 
  p <- ggplot(df1, aes(x = df1[,input.col]))+
  geom_bar(aes(color = more_than_9_NA, fill = more_than_9_NA), alpha = 0.4, show.legend = TRUE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(df1)[input.col]))+
  theme_classic()
return(p)
}

all.density.plot <- lapply(c(3,9), plot_NA_fun)

multiplot(plotlist = all.density.plot, cols = 1)

```
After analysis we found suspects behaviors like all those observations are only recorded 1992 and later (but very few others observations are recorded after 1991) or also paper_type = super is also only found in those observations. It seems that for those datas there is an obvious sample biais. Is datas with sample biais are included in the model.....

Thus we will ignore observations with more than 9 NAs to run our model. Accuracy will maybe be lower but we believe models will describe a better representation of the reality.  

```{r, echo = FALSE, warning=FALSE}

df <- df_few_NA

```


###Imputation


Theorically there are two ways to deal with missing values.

The first option is to ignore observations with missing values and the second strategy is to consider imputation of missing values. However, there is a serious problem with this first method. Ignoring missing values would reduce the dataset size which is already very small.
In our case we will proceed to a missing data imputation, which could be defined as a procedure where missing values are replaced with some plausible values. There are several methods to do a data imputation but there a two main categories.
The first one is the single imputation, where one plausible value is imputed to replace the missing value. The kNN imputation will be test in this report. The principle of this technique is to imput a values to the missing value from a similar record in the dataset.
The second option consist in replacing missing values by multiples missing values [@Jadhav]


```{r, include= FALSE, echo = FALSE, warning=FALSE}


#knn imputation with mod for factor and median for continuous 

df_knnimput <- knnImputation(df_few_NA, meth = "median")  # perform knn mputation. Think the rename de_knnimp




```

After this transformation, we do not have anymore missing values. This way to modify non available data is not the only approach that could be taken. We could have performed a mice imputation which is based on random forest.

```{r,warning=FALSE, echo=FALSE}



#multiple imputation
tempData <- mice(df_few_NA,m=5,maxit=11,meth='cart',seed=500, ntree =10 )
summary(tempData)
df <- mice::complete(tempData)


```




## Variables and feature selection

It may be thought that wider data set are providing better prediction, but this is totally wrong. In fact, many machine learning algortihms decrease in accuracy if the number of variable is signficantly higher than the optimal number of feature [@Kohavi:1997:WFS:270613.270627]. We will try to reduce a bit the number of variables using to techniques: 

To start this data selection process we could do a very trivial analyisis, based on ranking with correlation coefficient. But as seen above, there are not really large correlation coefficient, therefore no variable may be deleted in that way.



###Learning Vector Quantization 

The first algorithm, is the Learning Vector Quantization algorithm which is an artificial neural network algorithm.

```{r, warning= FALSE, echo=FALSE}



#Learning Vector Quantization

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=7)
# train the model
model <- train(band_type~., data=df, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# plot importance
plot(importance)



df_fewNA_cart_lvq_drop_11 <- df %>% select(-c("blade_pressure", "solvent_type","cylinder_size", "blade_mfg", "direct_steam","ESA_amperage","plating_tank","customer","day","nbr_of_NA","chrome_content"))


df_fewNA_cart_lvq_drop_15 <- df %>% select(-c("blade_pressure", "solvent_type","cylinder_size", "blade_mfg", "direct_steam","ESA_amperage","plating_tank","customer","day","nbr_of_NA","proof_ink","chrome_content","anode_space_ratio",
                                              "solvent_pct","ESA_voltage","wax"))





```
From the Learning Vector Quantization variable selection, we could delete variables up to **nbr_of_NA** and maybe higher. To confirm those result we will use a second method: Boruta. 

###Boruta 

Boruta is an algorithm, with an iterative construction, based on random forest, which helps us to achieve a data selection. The main advantage with Boruta is that it is providing an unbiased and stable selection of important and non-important attributes [@RePEc:jss:jstsof:v:036:i11]. 

```{r, warning=FALSE, echo=FALSE}

# to ensure reproducibility
set.seed(10)

boruta_df <- Boruta(band_type~.,data = df, doTrace = 2, ntree =500, maxRuns=500)
 
plot(boruta_df, xlab = "", xaxt = "n")
lz<-lapply(1:ncol(boruta_df$ImpHistory),function(i)
boruta_df$ImpHistory[is.finite(boruta_df$ImpHistory[,i]),i])
names(lz) <- colnames(boruta_df$ImpHistory)
Labels <- sort(sapply(lz,median))
axis(side = 1,las=2,labels = names(Labels),
at = 1:ncol(boruta_df$ImpHistory), cex.axis = 0.7)



df_fewNA_mix<- df %>% select(-c( "solvent_type", "blade_mfg", "direct_steam","ESA_amperage","plating_tank","day","nbr_of_NA","chrome_content", "customer"))
```
From this second method we find similar results but not exactly so we decided to only delete variables that were ranked low in both selection and costumer variable that was low only in the first method. We decided to delete it nevertherless because after some exploration we found out that deleting it has almost no impact on the prediction.  

Thus we decided to delete **ESA_amperage**, **blade_mfg**,**direct_steam**,**solvent_type**,**plating_tank**,**nbr_of_NA**,**chrome_content** and **customer**.

                                              
This last part will create the two final datasets for the knn_imput, one which is full and one reduced as the recursive feature elimination shows us that the optimal number of variable is around 6-7.

```{r, include=FALSE, warning=FALSE, echo=FALSE}

#full_df_knnimput <- df_knnimput
#save(full_df_knnimput,file="full_df_knnimput.Rda")
#boruta_df_knnimput <- df_knnimput[c(9,10,24,17,6,4,33)]
#save(boruta_df_knnimput,file="boruta_df_knnimput.Rda")
#lvq_df_knnimput <- df_knnimput[c(9,4,6,29,8,5,33)]
#save(lvq_df_knnimput,file="lvq_df_knnimput.Rda")

```
# References

## **Articles**

@Jadhav
@Guyon:2003:IVF:944919.944968
@Kohavi:1997:WFS:270613.270627
@RePEc:jss:jstsof:v:036:i11


## **Books**

@Provost:2013:DSB:2564781