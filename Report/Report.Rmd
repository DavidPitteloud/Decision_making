---
title: "Report"
author: "Leonard Philippossian David Pitteloud"
date: "11/10/2019"
output: 
 rmdformats::readthedown:
    css: custom.css
    self_contained: true
    toc_depth: 3
bibliography: bibliography.bibtex
---

```{r, include=FALSE, warning=FALSE, echo=FALSE}
#Put here all the required library

library("readr")
library("stringr")
library("lubridate")
library("explore")
library("MASS")
library("DT")
library("Hmisc")  
library("tidyverse")
library("ggthemes")
library("RColorBrewer")
library("corrplot")
library("psych")
library("GGally")
library("corrr")
library("ggcorrplot")
library("kableExtra")
library("DataExplorer")
library("inspectdf")
library("ggpubr")
library("gridExtra")
library("Rmisc")
library("OutliersO3")
library("e1071")
library("DMwR")
library("Boruta")
library("AppliedPredictiveModeling")
library("caret")
library("rpart")
library("randomForest")
library("neuralnet")
library("naniar")
library("ISLR")
library("leaps")
library("car")
```



```{r}
#import the database

bands3 <- read_delim("bands3.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)
#create data frame

df <- as.data.frame(bands3)
```



```{r}
#data exploration and variable selection

#separate date and transform it to day to day

df$date <- ymd(df$date)	

#add weekday variable 	
df$day <- weekdays(as.Date(df$date))
df$month <-month(as.Date(df$date))	
df$year <- year(as.Date(df$date))

#reorder variables, et j'ai juste enlever la date parce que ça me fait tout buguer plus bas
df <- df[c(41,42,43,2:40)]


```


```{r}
#data visualisations and cleaning

#replace all the symbol ? by NA
# Est ce qu'on doit vraiment le faire ça?
df[df == "?"] <- NA
#delete variable nbr 32 33 (original dtb)(voltage and amperage)

#delete all uselss columns 
#delete date, cylinder number, customer, job number, ink color,blade_fmg , cylinder division, unit number, #plating tank, esa_voltage, esa_amperage, chrome content

#df <- df[,-c(1,2,3,4,6,8,9,17,20,32,33,39)]
#ça on verra plus tard avec la selection
#moins de 20% de NA on garde la ligne

df<- df[!rowSums(is.na(df))>round((ncol(df)/100)*20),]


#remove capital letter
df <-data.frame(lapply(df, function(v) {
  if (is.character(v)) return(tolower(v))
  else return(v)
}))

#change format of columns 
#df[,c(1:11, 28)] <- lapply(df[,c(1:11, 28)], as.factor)
#df[,12:27] <- lapply(df[,12:27], as.numeric)
# ce n'est plus valable il faut pluto écrire comme ça
df[,c(1,4:22,42)] <- lapply(df[,c(1,4:22, 42)], as.factor)
df[,c(2,3,23:41)] <- lapply(df[,c(2,3,23:41)], as.numeric)
#Missing data 

plot_missing(df)

#there is some missing data so we will imputate them 

#knn imputation with mod for factor and median for continuous 

df <- knnImputation(df, meth = "median")  # perform knn mputation.


#other possible method 
#library(mice)
#miceMod <- mice(BostonHousing[, !names(BostonHousing) %in% "medv"], method="rf")  # perform mice imputation, based on random forests.
#miceOutput <- complete(miceMod)  # generate the completed data.


#resume 
str(df)
summary(df)

```






```{r}
##exploration visuelle des données quantitatives 

featurePlot(x = df[, is.numeric(df)==TRUE], 
            y = df$band_type,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(4, 1), 
            auto.key = list(columns = 3))
```






```{r}
#exploration des données quantitatives 

quant.df <- df[,c(2,3,23:42)]


#NA exploration 

plot_missing(quant.df)

#distribution 

plot_histogram(quant.df)


plot_density_fun <- function(input.col, data = quant.df){ 
  p <- ggplot(data, aes(x = data[,input.col]))+
  geom_density(aes(color = band_type, fill = band_type), alpha = 0.4) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(quant.df)[input.col]))+
  theme_bw()
return(p)
}

all.density.plot <- lapply(c(1:3), plot_density_fun)

multiplot(plotlist = all.density.plot,layout=matrix(c(1,2,3,3), nrow=2, byrow=TRUE))



#Correlation

plot_correlation(quant.df, type= 'c', cor_args = list( 'use' = 'complete.obs'))

#cette ligne marche pas a cazse des na
cor.matrix <- rcorr(as.matrix(quant.df))

corrplot(cor.matrix$r, method = "square")

ggpairs(quant.df)




```


```{r}
#exploration des données factorielles  

fact.df <- df[,c(1,4:22, 42)]


#NA exploration 

plot_missing(fact.df)

#distribution 
#plot_histogram(fact.df)


#intéressant mais difficilement lisible

plot_bar_fun <- function(input.col, data = fact.df){ 
  p <- ggplot(data, aes(x = data[,input.col]))+
  geom_bar(aes(color = band_type, fill = band_type), alpha = 0.4, show.legend = FALSE) +
  scale_color_manual(values = c("#868686FF", "#EFC000FF"))+
  scale_fill_manual(values = c("#868686FF", "#EFC000FF"))+
  xlab(paste(colnames(data)[input.col]))+
  theme(legend.position = "none")+
  theme_bw()
return(p)
}

all.density.plot <- lapply(c(1:22), plot_bar_fun)

multiplot(plotlist = all.density.plot, cols = 5)

```




```{r}
#visualisation 




featurePlot(x = df[, 13:27], 
            y = df$band_type,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(4, 1), 
            auto.key = list(columns = 3))


pp <- preProcess(df[, -29], 
                     method = c("center", "scale", "YeoJohnson", 'nzv'))


transformed <- cbind(predict(pp, newdata = df[, -29]),"band_type"  #df$band_type)
head(transformed)


```

Data selection will have an impact on models accurary, more precisely on the preiction performance of the predictors, therefore it should be performed carefully. However, it is not the only purpose of data selection. To select effectively variables will also facilitate visualisation and comprehension of the database, or reduce the measurement and storage requirements [Guyon:2003:IVF:944919.944968].

There are many ways to perform a feature selection. The first option would be to filter variable by ranking them with the help of different cprreéatopm coefficients. This technique has many advantages such as simplicity, scalability, and good empirical success [Guyon:2003:IVF:944919.944968].

Before selecting variable it is interesting to check all the factorial variables that have only one level, or to many levels, and that could be straightly deleted. 

```{r}
#explore all the variables
summary(df)
#delete variable with only one level and factor with to many levels

df <- df[,-c(4,5,6,8,10,11,14,18)]



```

```{r}
df2 <- df
df2$band_type <-  as.numeric(df2$band_type)
model <- lm(band_type~.,df2[,1:34])
summary(model)
```


Well je suis un incapable qui ne sait pas faire fonctionner ça 
du coup on va passer au modèle suivant:

Regarder les variables qui sont hautement corrélées entre elles

```{r}
Identify highly correlated features in caret r packageR
# ensure the results are repeatable
set.seed(7)

# calculate correlation matrix
correlationMatrix <- cor(df[,17:35])
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)


```
rank variables 

```{r}

# ensure results are repeatable
set.seed(7)

# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(band_type~., data=df, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)

```



```{r}
# ensure the results are repeatable
set.seed(7)

# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(df[,c(1:3,5:11,13:35)], df[,36], sizes=c(1:8), rfeControl=control)


# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))

```
```{r}

```

```{r}
#feature selection 

B <- Boruta(band_type~. ,data = df, pValue = 0.001, maxRuns = 000)


df <- df[,!B$finalDecision=="Rejected"]



```




```{r}
#Create a train and a test set

library(caret)

set.seed(998)

inTraining <- createDataPartition(transformed$band_type, p = .75, list = FALSE)


training <- df[ inTraining,]
testing  <- df[-inTraining,]




fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)


gbmFit1 <- train(band_type ~ ., data = training, 
                 method = "rf", 
                 trControl = fitControl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)

predict(gbmFit1, newdata = testing)

fm <- gbmFit1$finalModel$

varImp(fm)
trellis.par.set(caretTheme())
plot(gbmFit1)  

cm = table(testing[,29], predict(gbmFit1, newdata = testing)) 

importance(fm)





fun_model <- function(x, cont_meth = "repeatedcv", model = "rf" ){
  
  
  inTraining <- createDataPartition(x$band_type, p = .75, list = FALSE)
  training <- df[ inTraining,]
  testing  <- df[-inTraining,]
  
  fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10)
 
  rf_grid <- expand.grid(mtry = c(1, 2, 3,5,7,10))
  
  Fit <- train(band_type ~ ., data = training, 
                 method = "nnet", 
                 trControl = fitControl
                 ## This last option is actually one
                 ## for gbm() that passes through)

  return(Fit$finalModel)
}


fun_model(df)



net.sqrt <- neuralnet(band_type ~ ., training, hidden=10, threshold=0.01)
print(net.sqrt)
```



```{r}

#proof of concept prediction svm


set.seed(101) 
sample <- sample.int(n = nrow(df), size = floor(.75*nrow(df)), replace = F)
train <- train[,-c(1:23,41)]
test  <- test[,-c(1:23,41)]



model.train <- svm(band_type~. , data = train,  kernel = "radial", cost = 10 , gamma = 0.01)

summary(model.train)

y.pred <- predict(model.train, test)

y.pred<-data.frame(y.pred)
row.names(y.pred)
cm = table(test[row.names(y.pred), 18], predict(model.train, est)) #

```


```{r}
#build the classification tree
classification_tree <- rpart(formula = band_type~., 
                             data = train_set,
                             method="class")

#evaluation of the model
class_prediction <- predict(object = classification_tree,
                            newdata = test_set,
                            type = "class")
confusionMatrix(data = class_prediction,
                reference = test_set$band_type)



```

```{r}
#build a random forest
set.seed(123) #for reproducibility
random_forest <- randomForest(formula = band_type~., 
                             data = train_set)

#evaluation of the model

err <- random_forest$err.rate
oob_err <- err[nrow(err), "OOB"]
# Plot the model trained 
plot(random_forest)

# Add a legend since it doesn't have one by default
legend(x = "right", 
       legend = colnames(err),
       fill = 1:ncol(err))


#other evaluation
rf_prediction <- predict(object = random_forest,
                            newdata = test_set,
                            type = "class")
cm<- confusionMatrix(data = rf_prediction,
                reference = test_set$band_type)

paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)

```

```{r}



```

```{r}



```

```{r}



```

```{r}



```

```{r}



```

```{r}



```

```{r}



```




# References
@Guyon:2003:IVF:944919.944968